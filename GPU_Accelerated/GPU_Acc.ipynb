{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_PIdbpW3jvZ",
        "outputId": "2dd42a48-2480-4797-eeed-d930644834d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Mar  6 19:14:09 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8              9W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "assert torch.cuda.is_available(), 'Switch to GPU Runtime: Runtime > Change runtime type > GPU'\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n"
      ],
      "metadata": {
        "id": "VL_SkjkY37OE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using Device:{device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGtWIBFq37Qo",
        "outputId": "9cbd14d1-7f99-4d19-f748-f61c1bf75215"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Device:cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])"
      ],
      "metadata": {
        "id": "N6firqNK37Te"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-zX9NSX37WX",
        "outputId": "c5203d67-5daa-46ac-8b0d-5e316caf6db2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 16.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 479kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.84MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 5.83MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle= True, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=128, shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "id": "2QGKYhVd37ZP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training Data Size:\", len(train_dataset))\n",
        "print(\"Test Data Size:\", len(test_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmtxjUrA6ylK",
        "outputId": "efd9a2df-eb56-44e8-9609-a3fcff5135f5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data Size: 60000\n",
            "Test Data Size: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MLP\n",
        "class MLP(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MLP, self).__init__()\n",
        "    self.flatten = torch.nn.Flatten()\n",
        "    self.fc1 = torch.nn.Linear(28*28, 512)\n",
        "    self.fc2 = torch.nn.Linear(512, 256)\n",
        "    self.fc3 = torch.nn.Linear(256, 10)\n",
        "    self.relu = torch.nn.ReLU()\n",
        "\n",
        "  def forward(self,x):\n",
        "    x= self.flatten(x)\n",
        "    x= self.relu(self.fc1(x))\n",
        "    x= self.relu(self.fc2(x))\n",
        "    x=self.fc3(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "nlb22cct37cH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLP().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n"
      ],
      "metadata": {
        "id": "xCYvogmm37es"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TRAIN\n",
        "def train_epoch(loader, model, optimizer, criterion, device):\n",
        "  model.train()\n",
        "  for batch_idx, (data,target) in enumerate(loader):\n",
        "    data,target = data.to(device), target.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output= model(data)\n",
        "    loss= criterion(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx%100 ==0:\n",
        "      print(f\"batch {batch_idx}, loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "1gbaGSBt37hk"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(3):\n",
        "  print(f\"Epoch{epoch+1}\")\n",
        "  train_epoch(train_loader, model, optimizer, criterion, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWyKdL6j37kb",
        "outputId": "0783450c-fc8b-4bf9-f904-f6ffd5e11c6b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch1\n",
            "batch 0, loss: 2.3042\n",
            "batch 100, loss: 0.2279\n",
            "batch 200, loss: 0.1798\n",
            "batch 300, loss: 0.1803\n",
            "batch 400, loss: 0.1344\n",
            "Epoch2\n",
            "batch 0, loss: 0.0255\n",
            "batch 100, loss: 0.1235\n",
            "batch 200, loss: 0.1149\n",
            "batch 300, loss: 0.0868\n",
            "batch 400, loss: 0.0436\n",
            "Epoch3\n",
            "batch 0, loss: 0.0540\n",
            "batch 100, loss: 0.0560\n",
            "batch 200, loss: 0.0170\n",
            "batch 300, loss: 0.0727\n",
            "batch 400, loss: 0.0887\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate\n",
        "model.eval()\n",
        "correct=0\n",
        "total=0\n",
        "with torch.no_grad():\n",
        "  for data, target in test_loader:\n",
        "    data, target= data.to(device), target.to(device)\n",
        "    outputs = model(data)\n",
        "    _,predicted = torch.max(outputs.data,1)\n",
        "    total+=target.size(0)\n",
        "    correct+=(predicted==target).sum().item()\n",
        "accuracy = 100*correct/total\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syUJ65WU37ni",
        "outputId": "0a9af002-8e6d-46ab-c958-9f19cb1a4bdb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 97.41%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False, num_workers=2)\n",
        "\n",
        "# GPU inference with multiple runs for stability\n",
        "model.eval()\n",
        "gpu_times = []\n",
        "for _ in range(5):  # Average over 5 runs\n",
        "    with torch.no_grad():\n",
        "        total_time = 0\n",
        "        for data, target in test_loader:\n",
        "            data = data.to(device)\n",
        "            torch.cuda.synchronize()  # Sync before timing\n",
        "            start_time = time.time()\n",
        "            outputs = model(data)\n",
        "            torch.cuda.synchronize()  # Sync after\n",
        "            total_time += time.time() - start_time\n",
        "    gpu_times.append(total_time)\n",
        "\n",
        "gpu_time = sum(gpu_times) / len(gpu_times)  # Average time\n",
        "gpu_throughput = len(test_dataset) / gpu_time\n",
        "print(f\"GPU inference time (avg): {gpu_time:.4f} seconds\")\n",
        "print(f\"GPU throughput: {gpu_throughput:.2f} images/second\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "i-XK1e_E37qZ",
        "outputId": "0542e5d2-55b3-4659-e3dd-e1bcb0358038"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-a4ffdf5af7b0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynchronize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Sync before timing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynchronize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Sync after\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mtotal_time\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-33004065c78c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cpu inference\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False, num_workers=2)\n",
        "cpu_device = torch.device('cpu')\n",
        "model_cpu=model.to(cpu_device)\n",
        "cpu_time=0\n",
        "with torch.no_grad():\n",
        "  for data, target in test_loader:\n",
        "    data = data.to(cpu_device)\n",
        "    start_time=time.time()\n",
        "    outputs = model_cpu(data)\n",
        "    cpu_time+= time.time() - start_time\n",
        "\n",
        "cpu_throughput = len(test_dataset)/cpu_time\n",
        "print(f\"CPU Inference Time:{cpu_time:.4f} secs\")\n",
        "print(f\"CPU Throughput:{cpu_throughput:.2f} images/sec\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOjV3rZJ37tj",
        "outputId": "2e071d3b-bbcf-4b01-eaa8-cab345794dc3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU Inference Time:0.2933 secs\n",
            "CPU Throughput:34099.70 images/sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "speedup = (gpu_throughput - cpu_throughput) / cpu_throughput * 100\n",
        "print(f\"GPU speedup over CPU: {speedup:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qr7V_Wc037wZ",
        "outputId": "984137fa-7cb5-4b8d-dd06-cd12a391225a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU speedup over CPU: 2492.66%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Save model\n",
        "torch.save(model.state_dict(), 'mnist_model_gpu.pth')\n",
        "!cp mnist_model_gpu.pth /content/drive/MyDrive/mnist_model_gpu.pth\n",
        "print(\"Model saved to Drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgWtNKMb37zi",
        "outputId": "5ff1276e-05ba-41e2-b26d-5024b31c1121"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Model saved to Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare test batch\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False, num_workers=2)\n",
        "\n",
        "# GPU inference with multiple runs for stability\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "gpu_times = []\n",
        "for _ in range(5):  # Average over 5 runs\n",
        "    with torch.no_grad():\n",
        "        total_time = 0\n",
        "        for data, target in test_loader:\n",
        "            data = data.to(device)\n",
        "            torch.cuda.synchronize()  # Sync before timing\n",
        "            start_time = time.time()\n",
        "            outputs = model(data)\n",
        "            torch.cuda.synchronize()  # Sync after\n",
        "            total_time += time.time() - start_time\n",
        "    gpu_times.append(total_time)\n",
        "\n",
        "gpu_time = sum(gpu_times) / len(gpu_times)  # Average time\n",
        "gpu_throughput = len(test_dataset) / gpu_time\n",
        "print(f\"GPU inference time (avg): {gpu_time:.4f} seconds\")\n",
        "print(f\"GPU throughput: {gpu_throughput:.2f} images/second\")\n",
        "\n",
        "# CPU inference (unchanged)\n",
        "cpu_device = torch.device('cpu')\n",
        "model_cpu = model.to(cpu_device)\n",
        "cpu_time = 0\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        data = data.to(cpu_device)\n",
        "        start_time = time.time()\n",
        "        outputs = model_cpu(data)\n",
        "        cpu_time += time.time() - start_time\n",
        "\n",
        "cpu_throughput = len(test_dataset) / cpu_time\n",
        "print(f\"CPU inference time: {cpu_time:.4f} seconds\")\n",
        "print(f\"CPU throughput: {cpu_throughput:.2f} images/second\")\n",
        "\n",
        "# Speedup\n",
        "speedup = (gpu_throughput - cpu_throughput) / cpu_throughput * 100\n",
        "print(f\"GPU speedup over CPU: {speedup:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqwmnR5t372r",
        "outputId": "ee3ce52f-c9b4-43b6-ab1c-a89e9b7548a4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU inference time (avg): 0.0101 seconds\n",
            "GPU throughput: 993844.96 images/second\n",
            "CPU inference time: 0.2825 seconds\n",
            "CPU throughput: 35399.42 images/second\n",
            "GPU speedup over CPU: 2707.52%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Single-run sanity check with full test set in one batch\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=10000, shuffle=False, num_workers=2)\n",
        "model= model.to(device)\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    data, target = next(iter(test_loader))  # One batch of 10,000\n",
        "    data = data.to(device)\n",
        "    torch.cuda.synchronize()\n",
        "    start_time = time.time()\n",
        "    outputs = model(data)\n",
        "    torch.cuda.synchronize()\n",
        "    gpu_time = time.time() - start_time\n",
        "\n",
        "gpu_throughput = len(test_dataset) / gpu_time\n",
        "print(f\"GPU inference time (single run, 10k batch): {gpu_time:.4f} seconds\")\n",
        "print(f\"GPU throughput: {gpu_throughput:.2f} images/second\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Khk15ftUDAY7",
        "outputId": "6d216722-5792-413c-a232-a052ee28d50e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU inference time (single run, 10k batch): 0.0046 seconds\n",
            "GPU throughput: 2159118.71 images/second\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-run sanity check with 10k batch\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=10000, shuffle=False, num_workers=2)\n",
        "model= model.to(device)\n",
        "model.eval()\n",
        "\n",
        "gpu_times = []\n",
        "for _ in range(5):  # 5 runs\n",
        "    with torch.no_grad():\n",
        "        data, target = next(iter(test_loader))\n",
        "        data = data.to(device)\n",
        "        torch.cuda.synchronize()\n",
        "        start_time = time.time()\n",
        "        outputs = model(data)\n",
        "        torch.cuda.synchronize()\n",
        "        gpu_times.append(time.time() - start_time)\n",
        "\n",
        "gpu_time = sum(gpu_times) / 5\n",
        "gpu_throughput = len(test_dataset) / gpu_time\n",
        "print(f\"GPU inference time (avg, 10k batch): {gpu_time:.4f} seconds\")\n",
        "print(f\"GPU throughput: {gpu_throughput:.2f} images/second\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0J7H0ym4ClDG",
        "outputId": "6fcd6206-c9b7-4333-a9f7-7026183566a2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU inference time (avg, 10k batch): 0.0046 seconds\n",
            "GPU throughput: 2156875.89 images/second\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kqtlrClWEDiX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LnoFHb_V375i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qoYGarUM378q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "58dSnuRL37_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xpGKGxE038Cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7a89Ba4J38Fw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K45uShmX38Io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "es7R-JCV38Lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0U57mtBB38OY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}